{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Assignment1_training_sweep_Fashion_MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4ad01945dbe34164b1ea55bc0a8d35b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a76f260f70174e239e8df4f51870e8b1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a500e3b790764cb797c716d85b825690",
              "IPY_MODEL_f2ae58b266904f609838c565cf73b000"
            ]
          }
        },
        "a76f260f70174e239e8df4f51870e8b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a500e3b790764cb797c716d85b825690": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_2a7390ef9e294c7da2295de51a2cabf6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0.01MB of 0.01MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_da2be1b1c0044fa7aabcde86a00f3504"
          }
        },
        "f2ae58b266904f609838c565cf73b000": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_07fee6e5cbdf461c873ec1b6c0264745",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8d252ef4788c4fbca1efb0cc80387037"
          }
        },
        "2a7390ef9e294c7da2295de51a2cabf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "da2be1b1c0044fa7aabcde86a00f3504": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "07fee6e5cbdf461c873ec1b6c0264745": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8d252ef4788c4fbca1efb0cc80387037": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4868d2fc6aaf4d99899d16fa74dcb1de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e446b72472ea459e910ed09a36bb8edc",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3787012cdf3b42109eb95ea283a217ff",
              "IPY_MODEL_cf6f0b0af50b4566beb3770f1982f168"
            ]
          }
        },
        "e446b72472ea459e910ed09a36bb8edc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3787012cdf3b42109eb95ea283a217ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_1b9e63e7edba4a41999bda5560044917",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0.01MB of 0.01MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_56ad546a9dc04d54a65e911b3604d26d"
          }
        },
        "cf6f0b0af50b4566beb3770f1982f168": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7a5455cfd012477e8da7e10bb5c1c706",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7769300a85c04924a55d337708df8880"
          }
        },
        "1b9e63e7edba4a41999bda5560044917": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "56ad546a9dc04d54a65e911b3604d26d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7a5455cfd012477e8da7e10bb5c1c706": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7769300a85c04924a55d337708df8880": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lavakumar31/DL/blob/main/Copy_of_Assignment1_training_sweep_Fashion_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AABknAuK_8UD",
        "outputId": "f2abb6c0-9bc5-4459-9f2c-bffabca64b3f"
      },
      "source": [
        "!pip install wandb\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.12.10-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 5.1 MB/s \n",
            "\u001b[?25hCollecting yaspin>=1.0.0\n",
            "  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 52.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.6-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 57.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.10.0.2)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=5be5b9b1cc4878542496759aeddaf2ad2ceea9b6cf59ed1da1ac4e8696a468dc\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, yaspin, shortuuid, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.5.6 shortuuid-1.0.8 smmap-5.0.0 wandb-0.12.10 yaspin-2.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vuMLV7jAhpo"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1 + np.exp(-(z)))\n",
        "\n",
        "\n",
        "def tanh(z):\n",
        "    return np.tanh(z)\n",
        "\n",
        "\n",
        "def sin(z):\n",
        "    return np.sin(z)\n",
        "\n",
        "\n",
        "def relu(z):\n",
        "    return (z>0)*(z) + ((z<0)*(z)*0.01)\n",
        "   \n",
        "\n",
        "def softmax(Z):\n",
        "    return np.exp(Z) / np.sum(np.exp(Z))\n",
        "\n",
        "\n",
        "def der_sigmoid(z):\n",
        "    return  (1.0 / (1 + np.exp(-(z))))*(1 -  1.0 / (1 + np.exp(-(z))))\n",
        "\n",
        "def der_tanh(z):\n",
        "    return 1 - np.tanh(z) ** 2\n",
        "\n",
        "\n",
        "def der_relu(z):\n",
        "    return (z>0)*np.ones(z.shape) + (z<0)*(0.01*np.ones(z.shape) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Rcbw2UiAs_M"
      },
      "source": [
        "import numpy as np\n",
        "import scipy as sp\n",
        "import wandb\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "class FeedForwardNeuralNetwork:\n",
        "    def __init__(\n",
        "        self, \n",
        "        num_hidden_layers, \n",
        "        num_hidden_neurons, \n",
        "        X_train_raw, \n",
        "        Y_train_raw,  \n",
        "        N_train, \n",
        "        X_val_raw, \n",
        "        Y_val_raw, \n",
        "        N_val,\n",
        "        X_test_raw, \n",
        "        Y_test_raw, \n",
        "        N_test,        \n",
        "        optimizer,\n",
        "        batch_size,\n",
        "        weight_decay,\n",
        "        learning_rate,\n",
        "        max_epochs,\n",
        "        activation,\n",
        "        initializer,\n",
        "        loss\n",
        "\n",
        "    ):\n",
        "\n",
        "        \"\"\"\n",
        "        Here, we initialize the FeedForwardNeuralNetwork class with the number of hidden layers, number of hidden neurons, raw training data. \n",
        "        \"\"\"\n",
        "        \n",
        "        self.num_classes = np.max(Y_train_raw) + 1  # NUM_CLASSES\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.num_hidden_neurons = num_hidden_neurons\n",
        "        self.output_layer_size = self.num_classes\n",
        "        self.img_height = X_train_raw.shape[1]\n",
        "        self.img_width = X_train_raw.shape[2]\n",
        "        self.img_flattened_size = self.img_height * self.img_width\n",
        "\n",
        "        \n",
        "        self.layers = (\n",
        "            [self.img_flattened_size]\n",
        "            + num_hidden_layers * [num_hidden_neurons]\n",
        "            + [self.output_layer_size]\n",
        "        )\n",
        "\n",
        "        self.N_train = N_train\n",
        "        self.N_val = N_val\n",
        "        self.N_test = N_test\n",
        "        \n",
        "\n",
        "\n",
        "        self.X_train = np.transpose(\n",
        "            X_train_raw.reshape(\n",
        "                X_train_raw.shape[0], X_train_raw.shape[1] * X_train_raw.shape[2]\n",
        "            )\n",
        "        )  \n",
        "        self.X_test = np.transpose(\n",
        "            X_test_raw.reshape(\n",
        "                X_test_raw.shape[0], X_test_raw.shape[1] * X_test_raw.shape[2]\n",
        "            )\n",
        "        )  \n",
        "        self.X_val = np.transpose(\n",
        "            X_val_raw.reshape(\n",
        "                X_val_raw.shape[0], X_val_raw.shape[1] * X_val_raw.shape[2]\n",
        "            )\n",
        "        ) \n",
        "\n",
        "\n",
        "        self.X_train = self.X_train / 255\n",
        "        self.X_test = self.X_test / 255\n",
        "        self.X_val = self.X_val / 255\n",
        "        \n",
        "        self.Y_train = self.oneHotEncode(Y_train_raw)  \n",
        "        self.Y_val = self.oneHotEncode(Y_val_raw)\n",
        "        self.Y_test = self.oneHotEncode(Y_test_raw)\n",
        "       \n",
        "\n",
        "\n",
        "\n",
        "        self.Activations_dict = {\"SIGMOID\": sigmoid, \"TANH\": tanh, \"RELU\": relu}\n",
        "        self.DerActivation_dict = {\n",
        "            \"SIGMOID\": der_sigmoid,\n",
        "            \"TANH\": der_tanh,\n",
        "            \"RELU\": der_relu,\n",
        "        }\n",
        "\n",
        "        self.Initializer_dict = {\n",
        "            \"XAVIER\": self.Xavier_initializer,\n",
        "            \"RANDOM\": self.random_initializer,\n",
        "            \"HE\": self.He_initializer\n",
        "        }\n",
        "\n",
        "        self.Optimizer_dict = {\n",
        "            \"SGD\": self.sgdMiniBatch,\n",
        "            \"MGD\": self.mgd,\n",
        "            \"NAG\": self.nag,\n",
        "            \"RMSPROP\": self.rmsProp,\n",
        "            \"ADAM\": self.adam,\n",
        "            \"NADAM\": self.nadam,\n",
        "        }\n",
        "        \n",
        "        self.activation = self.Activations_dict[activation]\n",
        "        \n",
        "        self.der_activation = self.DerActivation_dict[activation]\n",
        "        \n",
        "        self.optimizer = self.Optimizer_dict[optimizer]\n",
        "        self.initializer = self.Initializer_dict[initializer]\n",
        "        self.loss_function = loss\n",
        "        self.max_epochs = max_epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "        self.weights, self.biases = self.initializeNeuralNet(self.layers)\n",
        "\n",
        "\n",
        "        \n",
        "        \n",
        "    # helper functions\n",
        "    def oneHotEncode(self, Y_train_raw):\n",
        "        Ydata = np.zeros((self.num_classes, Y_train_raw.shape[0]))\n",
        "        for i in range(Y_train_raw.shape[0]):\n",
        "            value = Y_train_raw[i]\n",
        "            Ydata[int(value)][i] = 1.0\n",
        "        return Ydata\n",
        "\n",
        "    # Loss functions\n",
        "    def meanSquaredErrorLoss(self, Y_true, Y_pred):\n",
        "        MSE = np.mean((Y_true - Y_pred) ** 2)\n",
        "        return MSE\n",
        "\n",
        "    def crossEntropyLoss(self, Y_true, Y_pred):\n",
        "        CE = [-Y_true[i] * np.log(Y_pred[i]) for i in range(len(Y_pred))]\n",
        "        crossEntropy = np.mean(CE)\n",
        "        return crossEntropy\n",
        "\n",
        "    def L2RegularisationLoss(self, weight_decay):\n",
        "        ALPHA = weight_decay\n",
        "        return ALPHA * np.sum(\n",
        "            [\n",
        "                np.linalg.norm(self.weights[str(i + 1)]) ** 2\n",
        "                for i in range(len(self.weights))\n",
        "            ]\n",
        "        )\n",
        "\n",
        "\n",
        "    def accuracy(self, Y_true, Y_pred, data_size):\n",
        "        Y_true_label = []\n",
        "        Y_pred_label = []\n",
        "        ctr = 0\n",
        "        for i in range(data_size):\n",
        "            Y_true_label.append(np.argmax(Y_true[:, i]))\n",
        "            Y_pred_label.append(np.argmax(Y_pred[:, i]))\n",
        "            if Y_true_label[i] == Y_pred_label[i]:\n",
        "                ctr += 1\n",
        "        accuracy = ctr / data_size\n",
        "        return accuracy, Y_true_label, Y_pred_label\n",
        "\n",
        "    def Xavier_initializer(self, size):\n",
        "        in_dim = size[1]\n",
        "        out_dim = size[0]\n",
        "        xavier_stddev = np.sqrt(2 / (in_dim + out_dim))\n",
        "        return np.random.normal(0, xavier_stddev, size=(out_dim, in_dim))\n",
        "\n",
        "    \n",
        "\n",
        "    def random_initializer(self, size):\n",
        "        in_dim = size[1]\n",
        "        out_dim = size[0]\n",
        "        return np.random.normal(0, 1, size=(out_dim, in_dim))\n",
        "\n",
        "\n",
        "    def He_initializer(self,size):\n",
        "        in_dim = size[1]\n",
        "        out_dim = size[0]\n",
        "        He_stddev = np.sqrt(2 / (in_dim))\n",
        "        return np.random.normal(0, 1, size=(out_dim, in_dim)) * He_stddev\n",
        "\n",
        "\n",
        "    def initializeNeuralNet(self, layers):\n",
        "        weights = {}\n",
        "        biases = {}\n",
        "        num_layers = len(layers)\n",
        "        for l in range(0, num_layers - 1):\n",
        "            W = self.initializer(size=[layers[l + 1], layers[l]])\n",
        "            b = np.zeros((layers[l + 1], 1))\n",
        "            weights[str(l + 1)] = W\n",
        "            biases[str(l + 1)] = b\n",
        "        return weights, biases\n",
        "\n",
        "    def forwardPropagate(self, X_train_batch, weights, biases):\n",
        "        \"\"\"\n",
        "        Returns the neural network given input data, weights, biases.\n",
        "        Arguments:\n",
        "                 : X - input matrix\n",
        "                 : Weights  - Weights matrix\n",
        "                 : biases - Bias vectors \n",
        "        \"\"\"\n",
        "        \n",
        "        num_layers = len(weights) + 1\n",
        "        # A - Preactivations\n",
        "        # H - Activations\n",
        "        X = X_train_batch\n",
        "        H = {}\n",
        "        A = {}\n",
        "        H[\"0\"] = X\n",
        "        A[\"0\"] = X\n",
        "        for l in range(0, num_layers - 2):\n",
        "            if l == 0:\n",
        "                W = weights[str(l + 1)]\n",
        "                b = biases[str(l + 1)]\n",
        "                A[str(l + 1)] = np.add(np.matmul(W, X), b)\n",
        "                H[str(l + 1)] = self.activation(A[str(l + 1)])\n",
        "            else:\n",
        "                W = weights[str(l + 1)]\n",
        "                b = biases[str(l + 1)]\n",
        "                A[str(l + 1)] = np.add(np.matmul(W, H[str(l)]), b)\n",
        "                H[str(l + 1)] = self.activation(A[str(l + 1)])\n",
        "\n",
        "        # Here the last layer is not activated as it is a regression problem\n",
        "        W = weights[str(num_layers - 1)]\n",
        "        b = biases[str(num_layers - 1)]\n",
        "        A[str(num_layers - 1)] = np.add(np.matmul(W, H[str(num_layers - 2)]), b)\n",
        "        # Y = softmax(A[-1])\n",
        "        Y = softmax(A[str(num_layers - 1)])\n",
        "        H[str(num_layers - 1)] = Y\n",
        "        return Y, H, A\n",
        "\n",
        "    def backPropagate(\n",
        "        self, Y, H, A, Y_train_batch, weight_decay=0\n",
        "    ):\n",
        "\n",
        "        ALPHA = weight_decay\n",
        "        gradients_weights = []\n",
        "        gradients_biases = []\n",
        "        num_layers = len(self.layers)\n",
        "\n",
        "        # Gradient with respect to the output layer is absolutely fine.\n",
        "        if self.loss_function == \"CROSS\":\n",
        "            globals()[\"grad_a\" + str(num_layers - 1)] = -(Y_train_batch - Y)\n",
        "        elif self.loss_function == \"MSE\":\n",
        "            globals()[\"grad_a\" + str(num_layers - 1)] = np.multiply(\n",
        "                2 * (Y - Y_train_batch), np.multiply(Y, (1 - Y))\n",
        "            )\n",
        "\n",
        "        for l in range(num_layers - 2, -1, -1):\n",
        "\n",
        "            if ALPHA != 0:\n",
        "                globals()[\"grad_W\" + str(l + 1)] = (\n",
        "                    np.outer(globals()[\"grad_a\" + str(l + 1)], H[str(l)])\n",
        "                    + ALPHA * self.weights[str(l + 1)]\n",
        "                )\n",
        "            elif ALPHA == 0:\n",
        "                globals()[\"grad_W\" + str(l + 1)] = np.outer(\n",
        "                    globals()[\"grad_a\" + str(l + 1)], H[str(l)]\n",
        "                )\n",
        "            globals()[\"grad_b\" + str(l + 1)] = globals()[\"grad_a\" + str(l + 1)]\n",
        "            gradients_weights.append(globals()[\"grad_W\" + str(l + 1)])\n",
        "            gradients_biases.append(globals()[\"grad_b\" + str(l + 1)])\n",
        "            if l != 0:\n",
        "                globals()[\"grad_h\" + str(l)] = np.matmul(\n",
        "                    self.weights[str(l + 1)].transpose(),\n",
        "                    globals()[\"grad_a\" + str(l + 1)],\n",
        "                )\n",
        "                globals()[\"grad_a\" + str(l)] = np.multiply(\n",
        "                    globals()[\"grad_h\" + str(l)], self.der_activation(A[str(l)])\n",
        "                )\n",
        "            elif l == 0:\n",
        "\n",
        "                globals()[\"grad_h\" + str(l)] = np.matmul(\n",
        "                    self.weights[str(l + 1)].transpose(),\n",
        "                    globals()[\"grad_a\" + str(l + 1)],\n",
        "                )\n",
        "                globals()[\"grad_a\" + str(l)] = np.multiply(\n",
        "                    globals()[\"grad_h\" + str(l)], (A[str(l)])\n",
        "                )\n",
        "        return gradients_weights, gradients_biases\n",
        "\n",
        "\n",
        "    def predict(self,X,length_dataset):\n",
        "        Y_pred = []        \n",
        "        for i in range(length_dataset):\n",
        "\n",
        "            Y, H, A = self.forwardPropagate(\n",
        "                X[:, i].reshape(self.img_flattened_size, 1),\n",
        "                self.weights,\n",
        "                self.biases,\n",
        "            )\n",
        "\n",
        "            Y_pred.append(Y.reshape(self.num_classes,))\n",
        "        Y_pred = np.array(Y_pred).transpose()\n",
        "        return Y_pred\n",
        "\n",
        "    def sgd(self, epochs, length_dataset, learning_rate, weight_decay=0):\n",
        "        \n",
        "        trainingloss = []\n",
        "        trainingaccuracy = []\n",
        "        validationaccuracy = []\n",
        "        \n",
        "        num_layers = len(self.layers)\n",
        "\n",
        "        X_train = self.X_train[:, :length_dataset]\n",
        "        Y_train = self.Y_train[:, :length_dataset]\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            start_time = time.time()\n",
        "           \n",
        "            idx = np.random.shuffle(np.arange(length_dataset))\n",
        "            X_train = X_train[:, idx].reshape(self.img_flattened_size, length_dataset)\n",
        "            Y_train = Y_train[:, idx].reshape(self.num_classes, length_dataset)\n",
        "\n",
        "            CE = []\n",
        "           \n",
        "            deltaw = [\n",
        "                np.zeros((self.layers[l + 1], self.layers[l]))\n",
        "                for l in range(0, len(self.layers) - 1)\n",
        "            ]\n",
        "            deltab = [\n",
        "                np.zeros((self.layers[l + 1], 1))\n",
        "                for l in range(0, len(self.layers) - 1)\n",
        "            ]\n",
        "\n",
        "            for i in range(length_dataset):\n",
        "\n",
        "                Y, H, A = self.forwardPropagate(\n",
        "                    X_train[:, i].reshape(self.img_flattened_size, 1),\n",
        "                    self.weights,\n",
        "                    self.biases,\n",
        "                )\n",
        "                grad_weights, grad_biases = self.backPropagate(\n",
        "                    Y, H, A, Y_train[:, i].reshape(self.num_classes, 1)\n",
        "                )\n",
        "                deltaw = [\n",
        "                    grad_weights[num_layers - 2 - i] for i in range(num_layers - 1)\n",
        "                ]\n",
        "                deltab = [\n",
        "                    grad_biases[num_layers - 2 - i] for i in range(num_layers - 1)\n",
        "                ]\n",
        "\n",
        "                \n",
        "\n",
        "                CE.append(\n",
        "                    self.crossEntropyLoss(\n",
        "                        self.Y_train[:, i].reshape(self.num_classes, 1), Y\n",
        "                    )\n",
        "                    + self.L2RegularisationLoss(weight_decay)\n",
        "                )\n",
        "\n",
        "                \n",
        "                self.weights = {\n",
        "                    str(i + 1): (self.weights[str(i + 1)] - learning_rate * deltaw[i])\n",
        "                    for i in range(len(self.weights))\n",
        "                }\n",
        "                self.biases = {\n",
        "                    str(i + 1): (self.biases[str(i + 1)] - learning_rate * deltab[i])\n",
        "                    for i in range(len(self.biases))\n",
        "                }\n",
        "\n",
        "            elapsed = time.time() - start_time\n",
        "            \n",
        "            Y_pred = self.predict(self.X_train, self.N_train)\n",
        "            trainingloss.append(np.mean(CE))\n",
        "            trainingaccuracy.append(self.accuracy(Y_train, Y_pred, length_dataset)[0])\n",
        "            validationaccuracy.append(self.accuracy(self.Y_val, self.predict(self.X_val, self.N_val), self.N_val)[0])\n",
        "            \n",
        "            print(\n",
        "                        \"Epoch: %d, Loss: %.3e, Training accuracy:%.2f, Validation Accuracy: %.2f, Time: %.2f, Learning Rate: %.3e\"\n",
        "                        % (\n",
        "                            epoch,\n",
        "                            trainingloss[epoch],\n",
        "                            trainingaccuracy[epoch],\n",
        "                            validationaccuracy[epoch],\n",
        "                            elapsed,\n",
        "                            self.learning_rate,\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "            wandb.log({'loss':np.mean(CE), 'trainingaccuracy':trainingaccuracy[epoch], 'validationaccuracy':validationaccuracy[epoch],'epoch':epoch, })\n",
        "       \n",
        "        return trainingloss, trainingaccuracy, validationaccuracy, Y_pred\n",
        "\n",
        "\n",
        "      \n",
        "    def sgdMiniBatch(self, epochs,length_dataset, batch_size, learning_rate, weight_decay = 0):\n",
        "\n",
        "        X_train = self.X_train[:, :length_dataset]\n",
        "        Y_train = self.Y_train[:, :length_dataset]        \n",
        "\n",
        "        trainingloss = []\n",
        "        trainingaccuracy = []\n",
        "        validationaccuracy = []\n",
        "        \n",
        "        num_layers = len(self.layers)\n",
        "        num_points_seen = 0\n",
        "\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            start_time = time.time()\n",
        "            idx = np.random.shuffle(np.arange(length_dataset))\n",
        "            X_train = X_train[:, idx].reshape(self.img_flattened_size, length_dataset)\n",
        "            Y_train = Y_train[:, idx].reshape(self.num_classes, length_dataset)\n",
        "            \n",
        "            CE = []\n",
        "            \n",
        "            \n",
        "            deltaw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "            deltab = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "\n",
        "            for i in range(length_dataset):\n",
        "                \n",
        "                Y,H,A = self.forwardPropagate(X_train[:,i].reshape(self.img_flattened_size,1), self.weights, self.biases) \n",
        "                grad_weights, grad_biases = self.backPropagate(Y,H,A,Y_train[:,i].reshape(self.num_classes,1))\n",
        "                \n",
        "                deltaw = [grad_weights[num_layers-2 - i] + deltaw[i] for i in range(num_layers - 1)]\n",
        "                deltab = [grad_biases[num_layers-2 - i] + deltab[i] for i in range(num_layers - 1)]\n",
        "                \n",
        "                \n",
        "                CE.append(self.crossEntropyLoss(self.Y_train[:,i].reshape(self.num_classes,1), Y) + self.L2RegularisationLoss(weight_decay))\n",
        "                \n",
        "                num_points_seen +=1\n",
        "                \n",
        "                if int(num_points_seen) % batch_size == 0:\n",
        "                    \n",
        "                    \n",
        "                    self.weights = {str(i+1):(self.weights[str(i+1)] - learning_rate*deltaw[i]/batch_size) for i in range(len(self.weights))} \n",
        "                    self.biases = {str(i+1):(self.biases[str(i+1)] - learning_rate*deltab[i]) for i in range(len(self.biases))}\n",
        "                    \n",
        "                    #resetting gradient updates\n",
        "                    deltaw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "                    deltab = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "            \n",
        "            elapsed = time.time() - start_time\n",
        "           \n",
        "            Y_pred = self.predict(self.X_train, self.N_train)\n",
        "            trainingloss.append(np.mean(CE))\n",
        "            trainingaccuracy.append(self.accuracy(Y_train, Y_pred, length_dataset)[0])\n",
        "            validationaccuracy.append(self.accuracy(self.Y_val, self.predict(self.X_val, self.N_val), self.N_val)[0])\n",
        "\n",
        "            print(\n",
        "                        \"Epoch: %d, Loss: %.3e, Training accuracy:%.2f, Validation Accuracy: %.2f, Time: %.2f, Learning Rate: %.3e\"\n",
        "                        % (\n",
        "                            epoch,\n",
        "                            trainingloss[epoch],\n",
        "                            trainingaccuracy[epoch],\n",
        "                            validationaccuracy[epoch],\n",
        "                            elapsed,\n",
        "                            self.learning_rate,\n",
        "                        )\n",
        "                    )\n",
        "                    \n",
        "            wandb.log({'loss':np.mean(CE), 'trainingaccuracy':trainingaccuracy[epoch], 'validationaccuracy':validationaccuracy[epoch],'epoch':epoch })\n",
        "            \n",
        "        return trainingloss, trainingaccuracy, validationaccuracy, Y_pred\n",
        "\n",
        "\n",
        "\n",
        "    def mgd(self, epochs,length_dataset, batch_size, learning_rate, weight_decay = 0):\n",
        "        GAMMA = 0.9\n",
        "\n",
        "        X_train = self.X_train[:, :length_dataset]\n",
        "        Y_train = self.Y_train[:, :length_dataset]        \n",
        "\n",
        "        \n",
        "        trainingloss = []\n",
        "        trainingaccuracy = []\n",
        "        validationaccuracy = []\n",
        "        \n",
        "        num_layers = len(self.layers)\n",
        "        prev_v_w = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "        prev_v_b = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "        num_points_seen = 0\n",
        "        for epoch in range(epochs):\n",
        "            start_time = time.time()\n",
        "            idx = np.random.shuffle(np.arange(length_dataset))\n",
        "            X_train = X_train[:, idx].reshape(self.img_flattened_size, length_dataset)\n",
        "            Y_train = Y_train[:, idx].reshape(self.num_classes, length_dataset)\n",
        "\n",
        "            CE = []\n",
        "            \n",
        "            deltaw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "            deltab = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "            \n",
        "\n",
        "            for i in range(length_dataset):\n",
        "                Y,H,A = self.forwardPropagate(self.X_train[:,i].reshape(self.img_flattened_size,1), self.weights, self.biases) \n",
        "                grad_weights, grad_biases = self.backPropagate(Y,H,A,self.Y_train[:,i].reshape(self.num_classes,1))\n",
        "                \n",
        "                deltaw = [grad_weights[num_layers-2 - i] + deltaw[i] for i in range(num_layers - 1)]\n",
        "                deltab = [grad_biases[num_layers-2 - i] + deltab[i] for i in range(num_layers - 1)]\n",
        "\n",
        "                CE.append(self.crossEntropyLoss(self.Y_train[:,i].reshape(self.num_classes,1), Y) + self.L2RegularisationLoss(weight_decay))\n",
        "                \n",
        "                num_points_seen +=1\n",
        "                \n",
        "                if int(num_points_seen) % batch_size == 0:\n",
        "\n",
        "                    v_w = [GAMMA*prev_v_w[i] + learning_rate*deltaw[i]/batch_size for i in range(num_layers - 1)]\n",
        "                    v_b = [GAMMA*prev_v_b[i] + learning_rate*deltab[i]/batch_size for i in range(num_layers - 1)]\n",
        "                    \n",
        "                    self.weights = {str(i+1) : (self.weights[str(i+1)] - v_w[i]) for i in range(len(self.weights))}\n",
        "                    self.biases = {str(i+1): (self.biases[str(i+1)] - v_b[i]) for i in range(len(self.biases))}\n",
        "\n",
        "                    prev_v_w = v_w\n",
        "                    prev_v_b = v_b\n",
        "\n",
        "                    #resetting gradient updates\n",
        "                    deltaw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "                    deltab = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "\n",
        "            elapsed = time.time() - start_time\n",
        "            Y_pred = self.predict(self.X_train, self.N_train)\n",
        "            trainingloss.append(np.mean(CE))\n",
        "            trainingaccuracy.append(self.accuracy(Y_train, Y_pred, length_dataset)[0])\n",
        "            validationaccuracy.append(self.accuracy(self.Y_val, self.predict(self.X_val, self.N_val), self.N_val)[0])\n",
        "\n",
        "            print(\n",
        "                        \"Epoch: %d, Loss: %.3e, Training accuracy:%.2f, Validation Accuracy: %.2f, Time: %.2f, Learning Rate: %.3e\"\n",
        "                        % (\n",
        "                            epoch,\n",
        "                            trainingloss[epoch],\n",
        "                            trainingaccuracy[epoch],\n",
        "                            validationaccuracy[epoch],\n",
        "                            elapsed,\n",
        "                            self.learning_rate,\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "            wandb.log({'loss':np.mean(CE), 'trainingaccuracy':trainingaccuracy[epoch], 'validationaccuracy':validationaccuracy[epoch],'epoch':epoch })\n",
        "\n",
        "\n",
        "        return trainingloss, trainingaccuracy, validationaccuracy, Y_pred\n",
        "\n",
        "\n",
        " \n",
        " \n",
        "    def stochasticNag(self,epochs,length_dataset, learning_rate, weight_decay = 0):\n",
        "        GAMMA = 0.9\n",
        "\n",
        "        X_train = self.X_train[:, :length_dataset]\n",
        "        Y_train = self.Y_train[:, :length_dataset]        \n",
        "\n",
        "        trainingloss = []\n",
        "        trainingaccuracy = []\n",
        "        validationaccuracy = []\n",
        "        \n",
        "        num_layers = len(self.layers)\n",
        "        \n",
        "        prev_v_w = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "        prev_v_b = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            start_time = time.time()\n",
        "            idx = np.random.shuffle(np.arange(length_dataset))\n",
        "            X_train = X_train[:, idx].reshape(self.img_flattened_size, length_dataset)\n",
        "            Y_train = Y_train[:, idx].reshape(self.num_classes, length_dataset)\n",
        "\n",
        "            CE = []\n",
        "            \n",
        "            deltaw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "            deltab = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "            \n",
        "            v_w = [GAMMA*prev_v_w[i] for i in range(0, len(self.layers)-1)]  \n",
        "            v_b = [GAMMA*prev_v_b[i] for i in range(0, len(self.layers)-1)]\n",
        "                        \n",
        "            for i in range(length_dataset):\n",
        "                winter = {str(i+1) : self.weights[str(i+1)] - v_w[i] for i in range(0, len(self.layers)-1)}\n",
        "                binter = {str(i+1) : self.biases[str(i+1)] - v_b[i] for i in range(0, len(self.layers)-1)}\n",
        "                \n",
        "                Y,H,A = self.forwardPropagate(self.X_train[:,i].reshape(self.img_flattened_size,1), winter, binter) \n",
        "                grad_weights, grad_biases = self.backPropagate(Y,H,A,self.Y_train[:,i].reshape(self.num_classes,1))\n",
        "                \n",
        "                deltaw = [grad_weights[num_layers-2 - i] for i in range(num_layers - 1)]\n",
        "                deltab = [grad_biases[num_layers-2 - i] for i in range(num_layers - 1)]\n",
        "\n",
        "                \n",
        "                CE.append(self.crossEntropyLoss(self.Y_train[:,i].reshape(self.num_classes,1), Y) + self.L2RegularisationLoss(weight_decay))\n",
        "                            \n",
        "                v_w = [GAMMA*prev_v_w[i] + learning_rate*deltaw[i] for i in range(num_layers - 1)]\n",
        "                v_b = [GAMMA*prev_v_b[i] + learning_rate*deltab[i] for i in range(num_layers - 1)]\n",
        "        \n",
        "                self.weights = {str(i+1):self.weights[str(i+1)] - v_w[i] for i in range(len(self.weights))} \n",
        "                self.biases = {str(i+1):self.biases[str(i+1)] - v_b[i] for i in range(len(self.biases))}\n",
        "                \n",
        "                prev_v_w = v_w\n",
        "                prev_v_b = v_b\n",
        "    \n",
        "            \n",
        "            elapsed = time.time() - start_time\n",
        "\n",
        "            Y_pred = self.predict(self.X_train, self.N_train)\n",
        "            trainingloss.append(np.mean(CE))\n",
        "            trainingaccuracy.append(self.accuracy(Y_train, Y_pred, length_dataset)[0])\n",
        "            validationaccuracy.append(self.accuracy(self.Y_val, self.predict(self.X_val, self.N_val), self.N_val)[0])\n",
        "\n",
        "            print(\n",
        "                        \"Epoch: %d, Loss: %.3e, Training accuracy:%.2f, Validation Accuracy: %.2f, Time: %.2f, Learning Rate: %.3e\"\n",
        "                        % (\n",
        "                            epoch,\n",
        "                            trainingloss[epoch],\n",
        "                            trainingaccuracy[epoch],\n",
        "                            validationaccuracy[epoch],\n",
        "                            elapsed,\n",
        "                            self.learning_rate,\n",
        "                        )\n",
        "                    )\n",
        "                    \n",
        "            wandb.log({'loss':np.mean(CE), 'trainingaccuracy':trainingaccuracy[epoch], 'validationaccuracy':validationaccuracy[epoch],'epoch':epoch })\n",
        "        \n",
        "        return trainingloss, trainingaccuracy, validationaccuracy, Y_pred\n",
        "    \n",
        "\n",
        "    def nag(self,epochs,length_dataset, batch_size,learning_rate, weight_decay = 0):\n",
        "        GAMMA = 0.9\n",
        "\n",
        "        X_train = self.X_train[:, :length_dataset]\n",
        "        Y_train = self.Y_train[:, :length_dataset]        \n",
        "\n",
        "\n",
        "        trainingloss = []\n",
        "        trainingaccuracy = []\n",
        "        validationaccuracy = []\n",
        "        \n",
        "        num_layers = len(self.layers)\n",
        "        \n",
        "        prev_v_w = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "        prev_v_b = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "        \n",
        "        num_points_seen = 0\n",
        "        for epoch in range(epochs):\n",
        "            start_time = time.time()\n",
        "            idx = np.random.shuffle(np.arange(length_dataset))\n",
        "            X_train = X_train[:, idx].reshape(self.img_flattened_size, length_dataset)\n",
        "            Y_train = Y_train[:, idx].reshape(self.num_classes, length_dataset)\n",
        "\n",
        "            CE = []\n",
        "           \n",
        "            \n",
        "            deltaw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "            deltab = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "            \n",
        "            v_w = [GAMMA*prev_v_w[i] for i in range(0, len(self.layers)-1)]  \n",
        "            v_b = [GAMMA*prev_v_b[i] for i in range(0, len(self.layers)-1)]\n",
        "\n",
        "            for i in range(length_dataset):\n",
        "                winter = {str(i+1) : self.weights[str(i+1)] - v_w[i] for i in range(0, len(self.layers)-1)}\n",
        "                binter = {str(i+1) : self.biases[str(i+1)] - v_b[i] for i in range(0, len(self.layers)-1)}\n",
        "                \n",
        "                Y,H,A = self.forwardPropagate(self.X_train[:,i].reshape(self.img_flattened_size,1), winter, binter) \n",
        "                grad_weights, grad_biases = self.backPropagate(Y,H,A,self.Y_train[:,i].reshape(self.num_classes,1))\n",
        "                \n",
        "                deltaw = [grad_weights[num_layers-2 - i] + deltaw[i] for i in range(num_layers - 1)]\n",
        "                deltab = [grad_biases[num_layers-2 - i] + deltab[i] for i in range(num_layers - 1)]\n",
        "\n",
        "                \n",
        "                CE.append(self.crossEntropyLoss(self.Y_train[:,i].reshape(self.num_classes,1), Y) + self.L2RegularisationLoss(weight_decay))\n",
        "\n",
        "                num_points_seen +=1\n",
        "                \n",
        "                if int(num_points_seen) % batch_size == 0:                            \n",
        "\n",
        "                    v_w = [GAMMA*prev_v_w[i] + learning_rate*deltaw[i]/batch_size for i in range(num_layers - 1)]\n",
        "                    v_b = [GAMMA*prev_v_b[i] + learning_rate*deltab[i]/batch_size for i in range(num_layers - 1)]\n",
        "        \n",
        "                    self.weights ={str(i+1):self.weights[str(i+1)]  - v_w[i] for i in range(len(self.weights))}\n",
        "                    self.biases = {str(i+1):self.biases[str(i+1)]  - v_b[i] for i in range(len(self.biases))}\n",
        "                \n",
        "                    prev_v_w = v_w\n",
        "                    prev_v_b = v_b\n",
        "\n",
        "                    deltaw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "                    deltab = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "\n",
        "    \n",
        "            \n",
        "            elapsed = time.time() - start_time\n",
        "            \n",
        "            Y_pred = self.predict(self.X_train, self.N_train)\n",
        "            trainingloss.append(np.mean(CE))\n",
        "            trainingaccuracy.append(self.accuracy(Y_train, Y_pred, length_dataset)[0])\n",
        "            validationaccuracy.append(self.accuracy(self.Y_val, self.predict(self.X_val, self.N_val), self.N_val)[0])\n",
        "\n",
        "            print(\n",
        "                        \"Epoch: %d, Loss: %.3e, Training accuracy:%.2f, Validation Accuracy: %.2f, Time: %.2f, Learning Rate: %.3e\"\n",
        "                        % (\n",
        "                            epoch,\n",
        "                            trainingloss[epoch],\n",
        "                            trainingaccuracy[epoch],\n",
        "                            validationaccuracy[epoch],\n",
        "                            elapsed,\n",
        "                            self.learning_rate,\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "            wandb.log({'loss':np.mean(CE), 'trainingaccuracy':trainingaccuracy[epoch], 'validationaccuracy':validationaccuracy[epoch],'epoch':epoch })\n",
        "        \n",
        "        return trainingloss, trainingaccuracy, validationaccuracy, Y_pred\n",
        "    \n",
        "\n",
        "    \n",
        "    def rmsProp(self, epochs,length_dataset, batch_size, learning_rate, weight_decay = 0):\n",
        "\n",
        "\n",
        "        X_train = self.X_train[:, :length_dataset]\n",
        "        Y_train = self.Y_train[:, :length_dataset]        \n",
        "\n",
        "        \n",
        "        trainingloss = []\n",
        "        trainingaccuracy = []\n",
        "        validationaccuracy = []\n",
        "        \n",
        "        num_layers = len(self.layers)\n",
        "        EPS, BETA = 1e-8, 0.9\n",
        "        \n",
        "        v_w = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "        v_b = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "        \n",
        "        num_points_seen = 0        \n",
        "        for epoch in range(epochs):\n",
        "            start_time = time.time()\n",
        "            idx = np.random.shuffle(np.arange(length_dataset))\n",
        "            X_train = X_train[:, idx].reshape(self.img_flattened_size, length_dataset)\n",
        "            Y_train = Y_train[:, idx].reshape(self.num_classes, length_dataset)\n",
        "\n",
        "\n",
        "            CE = []\n",
        "            \n",
        "                        \n",
        "            deltaw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "            deltab = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "\n",
        "            for i in range(length_dataset):\n",
        "            \n",
        "                Y,H,A = self.forwardPropagate(self.X_train[:,i].reshape(self.img_flattened_size,1), self.weights, self.biases) \n",
        "                grad_weights, grad_biases = self.backPropagate(Y,H,A,self.Y_train[:,i].reshape(self.num_classes,1))\n",
        "            \n",
        "                deltaw = [grad_weights[num_layers-2 - i] + deltaw[i] for i in range(num_layers - 1)]\n",
        "                deltab = [grad_biases[num_layers-2 - i] + deltab[i] for i in range(num_layers - 1)]\n",
        "                \n",
        "                \n",
        "                CE.append(self.crossEntropyLoss(self.Y_train[:,i].reshape(self.num_classes,1), Y) + self.L2RegularisationLoss(weight_decay))            \n",
        "                num_points_seen +=1\n",
        "                \n",
        "                if int(num_points_seen) % batch_size == 0:\n",
        "                \n",
        "                    v_w = [BETA*v_w[i] + (1-BETA)*(deltaw[i])**2 for i in range(num_layers - 1)]\n",
        "                    v_b = [BETA*v_b[i] + (1-BETA)*(deltab[i])**2 for i in range(num_layers - 1)]\n",
        "\n",
        "                    self.weights = {str(i+1):self.weights[str(i+1)]  - deltaw[i]*(learning_rate/np.sqrt(v_w[i]+EPS)) for i in range(len(self.weights))} \n",
        "                    self.biases = {str(i+1):self.biases[str(i+1)]  - deltab[i]*(learning_rate/np.sqrt(v_b[i]+EPS)) for i in range(len(self.biases))}\n",
        "\n",
        "                    deltaw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "                    deltab = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "    \n",
        "            \n",
        "            elapsed = time.time() - start_time\n",
        "           \n",
        "            Y_pred = self.predict(self.X_train, self.N_train)\n",
        "            trainingloss.append(np.mean(CE))\n",
        "            trainingaccuracy.append(self.accuracy(Y_train, Y_pred, length_dataset)[0])\n",
        "            validationaccuracy.append(self.accuracy(self.Y_val, self.predict(self.X_val, self.N_val), self.N_val)[0])\n",
        "\n",
        "            print(\n",
        "                        \"Epoch: %d, Loss: %.3e, Training accuracy:%.2f, Validation Accuracy: %.2f, Time: %.2f, Learning Rate: %.3e\"\n",
        "                        % (\n",
        "                            epoch,\n",
        "                            trainingloss[epoch],\n",
        "                            trainingaccuracy[epoch],\n",
        "                            validationaccuracy[epoch],\n",
        "                            elapsed,\n",
        "                            self.learning_rate,\n",
        "                        )\n",
        "                    )\n",
        "                    \n",
        "            wandb.log({'loss':np.mean(CE), 'trainingaccuracy':trainingaccuracy[epoch], 'validationaccuracy':validationaccuracy[epoch],'epoch':epoch })\n",
        "        \n",
        "        return trainingloss, trainingaccuracy, validationaccuracy, Y_pred  \n",
        "\n",
        "\n",
        "\n",
        "    def adam(self, epochs,length_dataset, batch_size, learning_rate, weight_decay = 0):\n",
        "        \n",
        "        X_train = self.X_train[:, :length_dataset]\n",
        "        Y_train = self.Y_train[:, :length_dataset]        \n",
        "\n",
        "        trainingloss = []\n",
        "        trainingaccuracy = []\n",
        "        validationaccuracy = []\n",
        "        num_layers = len(self.layers)\n",
        "        EPS, BETA1, BETA2 = 1e-8, 0.9, 0.99\n",
        "        \n",
        "        m_w = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "        m_b = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "        \n",
        "        v_w = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "        v_b = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]        \n",
        "        \n",
        "        m_w_hat = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "        m_b_hat = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "        \n",
        "        v_w_hat = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "        v_b_hat = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]   \n",
        "        \n",
        "        num_points_seen = 0 \n",
        "        for epoch in range(epochs):\n",
        "            start_time = time.time()\n",
        "            idx = np.random.shuffle(np.arange(length_dataset))\n",
        "            X_train = X_train[:, idx].reshape(self.img_flattened_size, length_dataset)\n",
        "            Y_train = Y_train[:, idx].reshape(self.num_classes, length_dataset)\n",
        "\n",
        "\n",
        "            CE = []\n",
        "            \n",
        "            \n",
        "            deltaw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "            deltab = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "            \n",
        "           \n",
        "            for i in range(length_dataset):\n",
        "                Y,H,A = self.forwardPropagate(self.X_train[:,i].reshape(self.img_flattened_size,1), self.weights, self.biases) \n",
        "                grad_weights, grad_biases = self.backPropagate(Y,H,A,self.Y_train[:,i].reshape(self.num_classes,1))\n",
        "                \n",
        "                deltaw = [grad_weights[num_layers-2 - i] + deltaw[i] for i in range(num_layers - 1)]\n",
        "                deltab = [grad_biases[num_layers-2 - i] + deltab[i] for i in range(num_layers - 1)]\n",
        "\n",
        "               \n",
        "                CE.append(self.crossEntropyLoss(self.Y_train[:,i].reshape(self.num_classes,1), Y) + self.L2RegularisationLoss(weight_decay))                 \n",
        "\n",
        "                num_points_seen += 1\n",
        "                ctr = 0\n",
        "                if int(num_points_seen) % batch_size == 0:\n",
        "                    ctr += 1\n",
        "                \n",
        "                    m_w = [BETA1*m_w[i] + (1-BETA1)*deltaw[i] for i in range(num_layers - 1)]\n",
        "                    m_b = [BETA1*m_b[i] + (1-BETA1)*deltab[i] for i in range(num_layers - 1)]\n",
        "                \n",
        "                    v_w = [BETA2*v_w[i] + (1-BETA2)*(deltaw[i])**2 for i in range(num_layers - 1)]\n",
        "                    v_b = [BETA2*v_b[i] + (1-BETA2)*(deltab[i])**2 for i in range(num_layers - 1)]\n",
        "                    \n",
        "                    m_w_hat = [m_w[i]/(1-BETA1**(epoch+1)) for i in range(num_layers - 1)]\n",
        "                    m_b_hat = [m_b[i]/(1-BETA1**(epoch+1)) for i in range(num_layers - 1)]            \n",
        "                \n",
        "                    v_w_hat = [v_w[i]/(1-BETA2**(epoch+1)) for i in range(num_layers - 1)]\n",
        "                    v_b_hat = [v_b[i]/(1-BETA2**(epoch+1)) for i in range(num_layers - 1)]\n",
        "                \n",
        "                    self.weights = {str(i+1):self.weights[str(i+1)] - (learning_rate/np.sqrt(v_w[i]+EPS))*m_w_hat[i] for i in range(len(self.weights))} \n",
        "                    self.biases = {str(i+1):self.biases[str(i+1)] - (learning_rate/np.sqrt(v_b[i]+EPS))*m_b_hat[i] for i in range(len(self.biases))}\n",
        "\n",
        "                    deltaw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "                    deltab = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "\n",
        "\n",
        "            elapsed = time.time() - start_time\n",
        "           \n",
        "            Y_pred = self.predict(self.X_train, self.N_train)\n",
        "            trainingloss.append(np.mean(CE))\n",
        "            trainingaccuracy.append(self.accuracy(Y_train, Y_pred, length_dataset)[0])\n",
        "            validationaccuracy.append(self.accuracy(self.Y_val, self.predict(self.X_val, self.N_val), self.N_val)[0])\n",
        "\n",
        "            print(\n",
        "                        \"Epoch: %d, Loss: %.3e, Training accuracy:%.2f, Validation Accuracy: %.2f, Time: %.2f, Learning Rate: %.3e\"\n",
        "                        % (\n",
        "                            epoch,\n",
        "                            trainingloss[epoch],\n",
        "                            trainingaccuracy[epoch],\n",
        "                            validationaccuracy[epoch],\n",
        "                            elapsed,\n",
        "                            self.learning_rate,\n",
        "                        )\n",
        "                    )\n",
        "                    \n",
        "            wandb.log({'loss':np.mean(CE), 'trainingaccuracy':trainingaccuracy[epoch], 'validationaccuracy':validationaccuracy[epoch],'epoch':epoch })\n",
        "        \n",
        "        return trainingloss, trainingaccuracy, validationaccuracy, Y_pred\n",
        "\n",
        "\n",
        "    \n",
        "    def nadam(self, epochs,length_dataset, batch_size, learning_rate, weight_decay = 0):\n",
        "\n",
        "        X_train = self.X_train[:, :length_dataset]\n",
        "        Y_train = self.Y_train[:, :length_dataset]        \n",
        "\n",
        "        \n",
        "        trainingloss = []\n",
        "        trainingaccuracy = []\n",
        "        validationaccuracy = []\n",
        "        num_layers = len(self.layers)\n",
        "        \n",
        "        GAMMA, EPS, BETA1, BETA2 = 0.9, 1e-8, 0.9, 0.99\n",
        "\n",
        "        m_w = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "        m_b = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "        \n",
        "        v_w = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "        v_b = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]        \n",
        "\n",
        "        m_w_hat = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "        m_b_hat = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "        \n",
        "        v_w_hat = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "        v_b_hat = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)] \n",
        "\n",
        "        num_points_seen = 0 \n",
        "        \n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            start_time = time.time()\n",
        "            idx = np.random.shuffle(np.arange(length_dataset))\n",
        "            X_train = X_train[:, idx].reshape(self.img_flattened_size, length_dataset)\n",
        "            Y_train = Y_train[:, idx].reshape(self.num_classes, length_dataset)\n",
        "\n",
        "            CE = []\n",
        "           \n",
        "\n",
        "            deltaw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "            deltab = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "\n",
        "            for i in range(length_dataset):\n",
        "\n",
        "                Y,H,A = self.forwardPropagate(self.X_train[:,i].reshape(self.img_flattened_size,1), self.weights, self.biases) \n",
        "                grad_weights, grad_biases = self.backPropagate(Y,H,A,self.Y_train[:,i].reshape(self.num_classes,1))\n",
        "\n",
        "                deltaw = [grad_weights[num_layers-2 - i] + deltaw[i] for i in range(num_layers - 1)]\n",
        "                deltab = [grad_biases[num_layers-2 - i] + deltab[i] for i in range(num_layers - 1)]\n",
        "\n",
        "                \n",
        "                CE.append(self.crossEntropyLoss(self.Y_train[:,i].reshape(self.num_classes,1), Y) + self.L2RegularisationLoss(weight_decay))   \n",
        "                num_points_seen += 1\n",
        "                \n",
        "                if num_points_seen % batch_size == 0:\n",
        "                    \n",
        "                    m_w = [BETA1*m_w[i] + (1-BETA1)*deltaw[i] for i in range(num_layers - 1)]\n",
        "                    m_b = [BETA1*m_b[i] + (1-BETA1)*deltab[i] for i in range(num_layers - 1)]\n",
        "                    \n",
        "                    v_w = [BETA2*v_w[i] + (1-BETA2)*(deltaw[i])**2 for i in range(num_layers - 1)]\n",
        "                    v_b = [BETA2*v_b[i] + (1-BETA2)*(deltab[i])**2 for i in range(num_layers - 1)]\n",
        "                    \n",
        "                    m_w_hat = [m_w[i]/(1-BETA1**(epoch+1)) for i in range(num_layers - 1)]\n",
        "                    m_b_hat = [m_b[i]/(1-BETA1**(epoch+1)) for i in range(num_layers - 1)]            \n",
        "                    \n",
        "                    v_w_hat = [v_w[i]/(1-BETA2**(epoch+1)) for i in range(num_layers - 1)]\n",
        "                    v_b_hat = [v_b[i]/(1-BETA2**(epoch+1)) for i in range(num_layers - 1)]\n",
        "                    \n",
        "                    self.weights = {str(i+1):self.weights[str(i+1)] - (learning_rate/(np.sqrt(v_w_hat[i])+EPS))*(BETA1*m_w_hat[i]+ (1-BETA1)*deltaw[i]) for i in range(len(self.weights))} \n",
        "                    self.biases = {str(i+1):self.biases[str(i+1)] - (learning_rate/(np.sqrt(v_b_hat[i])+EPS))*(BETA1*m_b_hat[i] + (1-BETA1)*deltab[i]) for i in range(len(self.biases))}\n",
        "\n",
        "                    deltaw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "                    deltab = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "             \n",
        "            elapsed = time.time() - start_time\n",
        "\n",
        "            \n",
        "            Y_pred = self.predict(self.X_train, self.N_train)\n",
        "            trainingloss.append(np.mean(CE))\n",
        "            trainingaccuracy.append(self.accuracy(Y_train, Y_pred, length_dataset)[0])\n",
        "            validationaccuracy.append(self.accuracy(self.Y_val, self.predict(self.X_val, self.N_val), self.N_val)[0])\n",
        "\n",
        "            print(\n",
        "                        \"Epoch: %d, Loss: %.3e, Training accuracy:%.2f, Validation Accuracy: %.2f, Time: %.2f, Learning Rate: %.3e\"\n",
        "                        % (\n",
        "                            epoch,\n",
        "                            trainingloss[epoch],\n",
        "                            trainingaccuracy[epoch],\n",
        "                            validationaccuracy[epoch],\n",
        "                            elapsed,\n",
        "                            self.learning_rate,\n",
        "                        )\n",
        "                    )\n",
        "            wandb.log({'loss':np.mean(CE), 'trainingaccuracy':trainingaccuracy[epoch], 'validationaccuracy':validationaccuracy[epoch],'epoch':epoch })\n",
        "            \n",
        "        return trainingloss, trainingaccuracy, validationaccuracy, Y_pred  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_3tVRXJA8Rv",
        "outputId": "9c57f0a4-511f-43da-b388-93dd4dabe3ad"
      },
      "source": [
        "#wandb \n",
        "import wandb\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "\n",
        "(trainIn, trainOut), (testIn, testOut) = fashion_mnist.load_data()\n",
        "\n",
        "N_train_full = trainOut.shape[0]\n",
        "N_train = int(0.9*N_train_full)\n",
        "N_validation = int(0.1 * trainOut.shape[0])\n",
        "N_test = testOut.shape[0]\n",
        "\n",
        "\n",
        "idx  = np.random.choice(trainOut.shape[0], N_train_full, replace=False)\n",
        "idx2 = np.random.choice(testOut.shape[0], N_test, replace=False)\n",
        "\n",
        "trainInFull = trainIn[idx, :]\n",
        "trainOutFull = trainOut[idx]\n",
        "\n",
        "trainIn = trainInFull[:N_train,:]\n",
        "trainOut = trainOutFull[:N_train]\n",
        "\n",
        "validIn = trainInFull[N_train:, :]\n",
        "validOut = trainOutFull[N_train:]    \n",
        "\n",
        "testIn = testIn[idx2, :]\n",
        "testOut = testOut[idx2]\n",
        "\n",
        "\n",
        "sweep_config = {\n",
        "  \"name\": \"Bayesian Sweep\",\n",
        "  \"method\": \"bayes\",\n",
        "  \"metric\":{\n",
        "  \"name\": \"validationaccuracy\",\n",
        "  \"goal\": \"maximize\"\n",
        "  },\n",
        "  \"parameters\": {\n",
        "        \"max_epochs\": {\n",
        "            \"values\": [5, 10]\n",
        "        },\n",
        "\n",
        "        \"initializer\": {\n",
        "            \"values\": [\"RANDOM\", \"XAVIER\", \"HE\"]\n",
        "        },\n",
        "\n",
        "        \"num_layers\": {\n",
        "            \"values\": [2, 3, 4]\n",
        "        },\n",
        "        \n",
        "        \n",
        "        \"num_hidden_neurons\": {\n",
        "            \"values\": [32, 64, 128]\n",
        "        },\n",
        "        \n",
        "        \"activation\": {\n",
        "            \"values\": [ 'TANH',  'SIGMOID', 'RELU']\n",
        "        },\n",
        "        \n",
        "        \"learning_rate\": {\n",
        "            \"values\": [0.001, 0.0001]\n",
        "        },\n",
        "        \n",
        "        \n",
        "        \"weight_decay\": {\n",
        "            \"values\": [0, 0.0005,0.5]\n",
        "        },\n",
        "        \n",
        "        \"optimizer\": {\n",
        "            \"values\": [\"SGD\", \"MGD\", \"NAG\", \"RMSPROP\", \"ADAM\",\"NADAM\"]\n",
        "        },\n",
        "                    \n",
        "        \"batch_size\": {\n",
        "            \"values\": [16, 32, 64]\n",
        "        }\n",
        "        \n",
        "        \n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config,project='DL_A1', entity='lava')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: jyyc75v7\n",
            "Sweep URL: https://wandb.ai/lava/DL_A1/sweeps/jyyc75v7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyC4fPR8Cwgp"
      },
      "source": [
        "def train():    \n",
        "    config_defaults = dict(\n",
        "            max_epochs=5,\n",
        "            num_hidden_layers=3,\n",
        "            num_hidden_neurons=32,\n",
        "            weight_decay=0,\n",
        "            learning_rate=1e-3,\n",
        "            optimizer=\"MGD\",\n",
        "            batch_size=16,\n",
        "            activation=\"TANH\",\n",
        "            initializer=\"XAVIER\",\n",
        "            loss=\"CROSS\",\n",
        "        )\n",
        "        \n",
        "    #wandb.init(config = config_defaults)\n",
        "    wandb.init(project='DL', entity='rahulsundar',config = config_defaults)\n",
        "\n",
        "\n",
        "    wandb.run.name = \"hl_\" + str(wandb.config.num_hidden_layers) + \"_hn_\" + str(wandb.config.num_hidden_neurons) + \"_opt_\" + wandb.config.optimizer + \"_act_\" + wandb.config.activation + \"_lr_\" + str(wandb.config.learning_rate) + \"_bs_\"+str(wandb.config.batch_size) + \"_init_\" + wandb.config.initializer + \"_ep_\"+ str(wandb.config.max_epochs)+ \"_l2_\" + str(wandb.config.weight_decay) \n",
        "    CONFIG = wandb.config\n",
        "\n",
        "\n",
        "    \n",
        "    #sweep_id = wandb.sweep(sweep_config)\n",
        "  \n",
        "\n",
        "    FFNN = FeedForwardNeuralNetwork(\n",
        "        num_hidden_layers=CONFIG.num_hidden_layers,\n",
        "        num_hidden_neurons=CONFIG.num_hidden_neurons,\n",
        "        X_train_raw=trainIn,\n",
        "        Y_train_raw=trainOut,\n",
        "        N_train = N_train,\n",
        "        X_val_raw = validIn,\n",
        "        Y_val_raw = validOut,\n",
        "        N_val = N_validation,\n",
        "        X_test_raw = testIn,\n",
        "        Y_test_raw = testOut,\n",
        "        N_test = N_test,\n",
        "        optimizer = CONFIG.optimizer,\n",
        "        batch_size = CONFIG.batch_size,\n",
        "        weight_decay = CONFIG.weight_decay,\n",
        "        learning_rate = CONFIG.learning_rate,\n",
        "        max_epochs = CONFIG.max_epochs,\n",
        "        activation = CONFIG.activation,\n",
        "        initializer = CONFIG.initializer,\n",
        "        loss = CONFIG.loss\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "    training_loss, trainingaccuracy, validationaccuracy, Y_pred_train = FFNN.optimizer(FFNN.max_epochs, FFNN.N_train, FFNN.batch_size, FFNN.learning_rate)\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 979,
          "referenced_widgets": [
            "4ad01945dbe34164b1ea55bc0a8d35b2",
            "a76f260f70174e239e8df4f51870e8b1",
            "a500e3b790764cb797c716d85b825690",
            "f2ae58b266904f609838c565cf73b000",
            "2a7390ef9e294c7da2295de51a2cabf6",
            "da2be1b1c0044fa7aabcde86a00f3504",
            "07fee6e5cbdf461c873ec1b6c0264745",
            "8d252ef4788c4fbca1efb0cc80387037"
          ]
        },
        "id": "z4EENrgtDTnu",
        "outputId": "4c2cac9c-60ba-4236-d519-7446786100ac"
      },
      "source": [
        "wandb.agent(sweep_id, train, count = 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: hlcm39et with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: RELU\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitializer: RANDOM\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_epochs: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_hidden_neurons: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: RMSPROP\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlava\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/lava/DL/runs/hlcm39et\" target=\"_blank\">firm-sweep-1</a></strong> to <a href=\"https://wandb.ai/lava/DL\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "Sweep page: <a href=\"https://wandb.ai/lava/DL/sweeps/m8mf5fk4\" target=\"_blank\">https://wandb.ai/lava/DL/sweeps/m8mf5fk4</a><br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: RuntimeWarning: overflow encountered in exp\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:138: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:138: RuntimeWarning: invalid value encountered in multiply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: nan, Training accuracy:0.10, Validation Accuracy: 0.10, Time: 56.05, Learning Rate: 1.000e-03\n",
            "Epoch: 1, Loss: nan, Training accuracy:0.10, Validation Accuracy: 0.10, Time: 57.14, Learning Rate: 1.000e-03\n",
            "Epoch: 2, Loss: nan, Training accuracy:0.10, Validation Accuracy: 0.10, Time: 58.74, Learning Rate: 1.000e-03\n",
            "Epoch: 3, Loss: nan, Training accuracy:0.10, Validation Accuracy: 0.10, Time: 63.26, Learning Rate: 1.000e-03\n",
            "Epoch: 4, Loss: nan, Training accuracy:0.10, Validation Accuracy: 0.10, Time: 56.04, Learning Rate: 1.000e-03\n",
            "Epoch: 5, Loss: nan, Training accuracy:0.10, Validation Accuracy: 0.10, Time: 56.37, Learning Rate: 1.000e-03\n",
            "Epoch: 6, Loss: nan, Training accuracy:0.10, Validation Accuracy: 0.10, Time: 56.64, Learning Rate: 1.000e-03\n",
            "Epoch: 7, Loss: nan, Training accuracy:0.10, Validation Accuracy: 0.10, Time: 56.23, Learning Rate: 1.000e-03\n",
            "Epoch: 8, Loss: nan, Training accuracy:0.10, Validation Accuracy: 0.10, Time: 56.17, Learning Rate: 1.000e-03\n",
            "Epoch: 9, Loss: nan, Training accuracy:0.10, Validation Accuracy: 0.10, Time: 66.30, Learning Rate: 1.000e-03\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 171... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4ad01945dbe34164b1ea55bc0a8d35b2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
              "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>loss</td><td></td></tr><tr><td>trainingaccuracy</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validationaccuracy</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
              "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>loss</td><td>nan</td></tr><tr><td>trainingaccuracy</td><td>0.09965</td></tr><tr><td>validationaccuracy</td><td>0.10317</td></tr></table>\n",
              "</div></div>\n",
              "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
              "<br/>Synced <strong style=\"color:#cdcd00\">firm-sweep-1</strong>: <a href=\"https://wandb.ai/lava/DL/runs/hlcm39et\" target=\"_blank\">https://wandb.ai/lava/DL/runs/hlcm39et</a><br/>\n",
              "Find logs at: <code>./wandb/run-20220225_074049-hlcm39et/logs</code><br/>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ws3nvejr with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: SIGMOID\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitializer: XAVIER\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_epochs: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_hidden_neurons: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: NAG\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/lava/DL/runs/ws3nvejr\" target=\"_blank\">autumn-sweep-2</a></strong> to <a href=\"https://wandb.ai/lava/DL\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "Sweep page: <a href=\"https://wandb.ai/lava/DL/sweeps/m8mf5fk4\" target=\"_blank\">https://wandb.ai/lava/DL/sweeps/m8mf5fk4</a><br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuiqRBuDUhIw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647,
          "referenced_widgets": [
            "4868d2fc6aaf4d99899d16fa74dcb1de",
            "e446b72472ea459e910ed09a36bb8edc",
            "3787012cdf3b42109eb95ea283a217ff",
            "cf6f0b0af50b4566beb3770f1982f168",
            "1b9e63e7edba4a41999bda5560044917",
            "56ad546a9dc04d54a65e911b3604d26d",
            "7a5455cfd012477e8da7e10bb5c1c706",
            "7769300a85c04924a55d337708df8880"
          ]
        },
        "outputId": "2ec1735d-16c7-409c-aca7-c5db51686635"
      },
      "source": [
        "wandb.finish()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-22:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/agents/pyagent.py\", line 302, in _run_job\n",
            "    self._function()\n",
            "  File \"<ipython-input-5-1200151d33a9>\", line 51, in train\n",
            "    training_loss, trainingaccuracy, validationaccuracy, Y_pred_train = FFNN.optimizer(FFNN.max_epochs, FFNN.N_train, FFNN.batch_size, FFNN.learning_rate)\n",
            "  File \"<ipython-input-3-c86bbe683f05>\", line 654, in nag\n",
            "    grad_weights, grad_biases = self.backPropagate(Y,H,A,self.Y_train[:,i].reshape(self.num_classes,1))\n",
            "  File \"<ipython-input-3-c86bbe683f05>\", line 277, in backPropagate\n",
            "    globals()[\"grad_a\" + str(l + 1)],\n",
            "Exception\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/agents/pyagent.py\", line 307, in _run_job\n",
            "    wandb.finish(exit_code=1)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/wandb_run.py\", line 2867, in finish\n",
            "    wandb.run.finish(exit_code=exit_code, quiet=quiet)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/wandb_run.py\", line 1472, in finish\n",
            "    hook.call()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/wandb_init.py\", line 326, in _jupyter_teardown\n",
            "    ipython.display_pub.publish = ipython.display_pub._orig_publish\n",
            "AttributeError: 'ZMQDisplayPublisher' object has no attribute '_orig_publish'\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 307... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4868d2fc6aaf4d99899d16fa74dcb1de",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
              "</div><div class=\"wandb-col\">\n",
              "</div></div>\n",
              "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
              "<br/>Synced <strong style=\"color:#cdcd00\">autumn-sweep-2</strong>: <a href=\"https://wandb.ai/lava/DL/runs/ws3nvejr\" target=\"_blank\">https://wandb.ai/lava/DL/runs/ws3nvejr</a><br/>\n",
              "Find logs at: <code>./wandb/run-20220225_075213-ws3nvejr/logs</code><br/>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}